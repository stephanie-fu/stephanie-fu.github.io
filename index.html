<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Stephanie Fu</title> <meta name="author" content="Stephanie Fu"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>☕</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://stephanie-fu.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Stephanie Fu </h1> <p class="desc">phd student | EECS @ UC Berkeley</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am a PhD student at BAIR advised by <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank" rel="noopener noreferrer">Trevor Darrell</a>.</p> <p>Previously, I graduated from MIT with an MEng in computer science (advised by Phillip Isola) and bachelors degrees in computer science and music. During my undergrad, I was fortunate to work on exciting research under <a href="https://billf.mit.edu/" target="_blank" rel="noopener noreferrer">Bill Freeman</a>, <a href="http://mit-pbg.mit.edu/" target="_blank" rel="noopener noreferrer">Yoel Fink</a>, and <a href="http://web.mit.edu/phillipi/" target="_blank" rel="noopener noreferrer">Phillip Isola</a>.</p> <p>I am broadly interested in the science behind deep learning and representation learning, especially in the realm of computer vision. Most recently, I have been interested in developing and understanding models with visual intelligence.</p> <hr> <p>In 2019, I co-founded <a href="https://tedx.mit.edu" target="_blank" rel="noopener noreferrer">TEDxMIT</a> and helped launch the inaugural conference at MIT CSAIL. Since then, TEDxMIT has brought communities across the Greater Boston area together with three more conferences and adapted to the COVID-19 pandemic by holding a virtual event. Check out the highlight reels from past events and details about the upcoming conference <a href="https://tedx.mit.edu/" target="_blank" rel="noopener noreferrer">here</a>!</p> <hr> </div> <div class="news"> <h2>news</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun 2023</th> <td> We just released <a href="https://dreamsim-nights.github.io" target="_blank" rel="noopener noreferrer">DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</a>! </td> </tr> <tr> <th scope="row">Jun 2023</th> <td> I will be attending UC Berkeley for my PhD in Computer Science, supported by the NSF GRFP and Chancellor’s Fellowship! </td> </tr> <tr> <th scope="row">May 2023</th> <td> I was offered Stanford’s <a href="https://knight-hennessy.stanford.edu/" target="_blank" rel="noopener noreferrer">Knight-Hennessy Scholarship</a>! </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/dreamsim_icon.png"></div> <div id="fu2023learning" class="col-sm-8"> <div class="title">DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</div> <div class="author"> <em>Stephanie Fu*</em>, Netanel Tamir*, Shobhita Sundaram*, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola</div> <div class="periodical"> <em>arXiv:2306.09344</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2306.09344" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/ssundaram21/dreamsim" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://dreamsim-nights.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/expl_icon.png"></div> <div id="hamilton2021axiomatic" class="col-sm-8"> <div class="title">Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning</div> <div class="author"> Mark Hamilton, Scott Lundberg, Lei Zhang, <em>Stephanie Fu</em>, and William T Freeman</div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2103.00370" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine’s behavior. We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate "fairness" and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained opaque-box models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/mosaic_icon.png"></div> <div id="pmlr-v133-hamilton21a" class="col-sm-8"> <div class="title">MosAIc: Finding Artistic Connections across Culture with Conditional Image Retrieval</div> <div class="author"> Mark Hamilton, <em>Stephanie Fu</em>, Mindren Lu, Johnny Bui, Darius Bopp, Zhenbang Chen, Felix Tran, Margaret Wang, Marina Rogers, Lei Zhang, Chris Hoder, and William T. Freeman</div> <div class="periodical"> <em>In Proceedings of the NeurIPS 2020 Competition and Demonstration Track</em> 06–12 dec 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://proceedings.mlr.press/v133/hamilton21a/hamilton21a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://www.microsoft.com/en-us/garage/blog/2020/08/mit-students-build-mosaic-to-explore-art-across-cultures-at-microsoft-garage/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://aka.ms/mosaic" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>We introduce MosAIc, an interactive web app that allows users to find pairs of semantically related artworks that span different cultures, media, and millennia. To create this application, we introduce Conditional Image Retrieval (CIR) which combines visual similarity search with user supplied filters or “conditions”. This technique allows one to find pairs of similar images that span distinct subsets of the image corpus. We provide a generic way to adapt existing image retrieval data-structures to this new domain and provide theoretical bounds on our approach’s efficiency. To quantify the performance of CIR systems, we introduce new datasets for evaluating CIR methods and show that CIR performs non-parametric style transfer. Finally, we demonstrate that our CIR data-structures can identify “blind spots” in Generative Adversarial Networks (GAN) where they fail to properly model the true data distribution.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/fibers_icon.png"></div> <div id="Loke2021" class="col-sm-8"> <div class="title">Digital electronics in fibres enable fabric-based machine-learning inference</div> <div class="author"> Gabriel Loke, Tural Khudiyev, Brian Wang, <em>Stephanie Fu</em>, Syamantak Payra, Yorai Shaoul, Johnny Fung, Ioannis Chatziveroglou, Pin-Wen Chou, Itamar Chinn, Wei Yan, Anna Gitelson-Kahn, John Joannopoulos, and Yoel Fink</div> <div class="periodical"> <em>Nature Communications</em> Jun 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1038/s41467-021-23628-5" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://news.mit.edu/2021/programmable-fiber-0603" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> </div> <div class="abstract hidden"> <p>Digital devices are the essential building blocks of any modern electronic system. Fibres containing digital devices could enable fabrics with digital system capabilities for applications in physiological monitoring, human-computer interfaces, and on-body machine-learning. Here, a scalable preform-to-fibre approach is used to produce tens of metres of flexible fibre containing hundreds of interspersed, digital temperature sensors and memory devices with a memory density of ~7.6\thinspace\texttimes\thinspace105 bits per metre. The entire ensemble of devices are individually addressable and independently operated through a single connection at the fibre edge, overcoming the perennial single-fibre single-device limitation and increasing system reliability. The digital fibre, when incorporated within a shirt, collects and stores body temperature data over multiple days, and enables real-time inference of wearer activity with an accuracy of 96% through a trained neural network with 1650 neuronal connections stored within the fibre. The ability to realise digital devices within a fibre strand which can not only measure and store physiological parameters, but also harbour the neural networks required to infer sensory data, presents intriguing opportunities for worn fabrics that sense, memorise, learn, and infer situational context.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%66%75%73@%63%73%61%69%6C.%6D%69%74.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://github.com/stephanie-fu" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/stephanie-fu" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/xkungfu" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> E-mail is usually the best way to reach me. </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Stephanie Fu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>